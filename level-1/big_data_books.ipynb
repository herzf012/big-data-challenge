{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dependences"
      ],
      "metadata": {
        "id": "_ipnIQO89Jpu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWUDVTFy83NW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.2.x  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "spark_version = 'spark-3.2.3'\n",
        "\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the PostgreSQL driver in our Colab environment\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "metadata": {
        "id": "NqeuUtz29EX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establish a Spark session and add the Postgres driver to the filepath\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"AMZreviews\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "metadata": {
        "id": "WpyBZFFt9GWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Data"
      ],
      "metadata": {
        "id": "NR2tE12F9QJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the first TSV.GZ file from an S3 bucket\n",
        "from pyspark import SparkFiles\n",
        "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_02.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "books_df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Books_v1_02.tsv.gz\"), sep=\"\\t\", header=True, inferSchema=True)\n",
        "\n",
        "# Show DataFrame\n",
        "books_df.show()"
      ],
      "metadata": {
        "id": "sDQmk_4a9VXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the amount of rows in the dataset\n",
        "print(f\"Rows in dataset: {books_df.count()}\")"
      ],
      "metadata": {
        "id": "4mp5KRlr9ZWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform Data"
      ],
      "metadata": {
        "id": "8lUHnCTg9azM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the amount of distinct rows in the dataset to find duplicates\n",
        "print(f\"Distinct rows in dataset: {books_df.distinct().count()}\")"
      ],
      "metadata": {
        "id": "Zf5YWcHJ9dOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unnecessary columns\n",
        "books_df = books_df.drop(\"marketplace\", \"product_category\", \"verified_purchase\", \"review_headline\", \"review_body\")\n",
        "\n",
        "books_df.show()"
      ],
      "metadata": {
        "id": "FnEwrGju9fat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find unique customer_id counts\n",
        "print(f\"Unique customer_id counts: {books_df.select('customer_id').distinct().count()}\")"
      ],
      "metadata": {
        "id": "sLi8vMKl9hJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find instances of each customer_id\n",
        "customer_id_count_df = books_df.groupBy('customer_id').count()\n",
        "\n",
        "books_df.show()"
      ],
      "metadata": {
        "id": "1GcCwMc69kpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join count to original dataframe\n",
        "books_df = books_df.join(customer_id_count_df, books_df.customer_id == customer_id_count_df.customer_id, \"left\").drop(books_df.customer_id)\n",
        "books_df.show()"
      ],
      "metadata": {
        "id": "58Tm-ji19m5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure rows are preserved\n",
        "print(books_df.count())"
      ],
      "metadata": {
        "id": "qXLZQfaf9nyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename count column\n",
        "books_df = books_df.withColumnRenamed(\"count\", \"customer_count\")\n",
        "books_df.show()"
      ],
      "metadata": {
        "id": "wpdlsSI-9p34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check data types\n",
        "books_df.printSchema()"
      ],
      "metadata": {
        "id": "zOF7gp8p9rsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "# Cast customer_count to integer and review_date to date\n",
        "\n",
        "books_df = books_df.withColumn(\"customer_count\", books_df.customer_count.cast(\"integer\"))\n",
        "books_df = books_df.withColumn(\"review_date\", books_df.review_date.cast(\"date\"))\n",
        "\n",
        "books_df.printSchema()"
      ],
      "metadata": {
        "id": "leQQxp0U9tzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create review_id dataframe for review_id_table in our database\n",
        "review_id_df = books_df.select(['review_id', 'customer_id', 'product_id', 'product_parent', 'review_date']).dropDuplicates()\n",
        "review_id_df.show() "
      ],
      "metadata": {
        "id": "swZuZTm-9xBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create products dataframe for products in our database\n",
        "products_df = books_df.select(['product_id', 'product_title']).dropDuplicates(['product_id'])\n",
        "products_df.show()"
      ],
      "metadata": {
        "id": "QgjhJUxE9zL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create customers dataframe for customers in our database\n",
        "customers_df = books_df.select(['customer_id', 'customer_count']).dropDuplicates()\n",
        "customers_df.show()"
      ],
      "metadata": {
        "id": "HbpAIu539z0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vine dataframe for vine_table in our database\n",
        "vine_df = books_df.select(['review_id', 'star_rating', 'helpful_votes', 'total_votes', 'vine']).dropDuplicates()\n",
        "vine_df.show()"
      ],
      "metadata": {
        "id": "KvIpQz0J93er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "BG4uyJHh96lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill out aws fields\n",
        "my_aws_endpoint = ''\n",
        "my_aws_port_number = '5432'\n",
        "my_aws_database_name = 'amazon_big_data_db'\n",
        "my_aws_username = 'postgres'\n",
        "my_aws_password = ''"
      ],
      "metadata": {
        "id": "0tJ9tWjs995M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the connection string\n",
        "jdbc_url=f'jdbc:postgresql://{my_aws_endpoint}:{my_aws_port_number}/{my_aws_database_name}'\n",
        "\n",
        "# Set up the configuration parameters\n",
        "config = {\"user\": f'{my_aws_username}', \n",
        "          \"password\": f'{my_aws_password}', \n",
        "          \"driver\":\"org.postgresql.Driver\"}\n",
        "\n",
        "# Choose to append the data\n",
        "mode = 'append'"
      ],
      "metadata": {
        "id": "h35hdHHp-AXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the dataframe to the appropriate table in our PostgreSQL RDS\n",
        "\n",
        "review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "1oxDbn04-D9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the dataframe to the appropriate table in our PostgreSQL RDS\n",
        "\n",
        "products_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "f-iEDk0_om65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the dataframe to the appropriate table in our PostgreSQL RDS\n",
        "\n",
        "customers_df.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "LZrxultXonON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the dataframe to the appropriate table in our PostgreSQL RDS\n",
        "\n",
        "vine_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "lSBDrq6moni7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}